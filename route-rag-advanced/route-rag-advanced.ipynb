{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a887192",
   "metadata": {},
   "source": [
    "#RAG Routing e self-querying com AstraDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necessárias\n",
    "import os\n",
    "from astrapy import DataAPIClient\n",
    "from astrapy.constants import VectorMetric\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d937c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar o modelo de incorporação (neste exemplo, o Graphdoc)\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "embed_model = FastEmbedEmbeddings(model_name=\"graphdoc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar o LLM (neste caso, usando Groq)\n",
    "from groq import Groq\n",
    "from langchain_groq import ChatGroq\n",
    "from google.colab import userdata\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperatura=0,\n",
    "    model_name=\"Llama3-8b-8192\",\n",
    "    api_key=userdata.get(\"GROQ_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12add62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs dos documentos a serem carregados\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Carregar documentos da web\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "print(f\"len de documentos: {len(docs_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fc6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir documentos em pedaços menores para se adequarem à janela de contexto do LLM\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=512, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "print(f\"Comprimento dos pedaços de documento gerados: {len(doc_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f7760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o cliente Astra\n",
    "client = DataAPIClient(os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"])\n",
    "database = client.get_database(os.environ[\"ASTRA_DB_API_ENDPOINT\"])\n",
    "print(f\"* Database: {database.info().name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52223af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma coleção para armazenar os vetores\n",
    "collection = database.create_collection(\n",
    "    \"vector_test\",\n",
    "    dimension=768,  # Certifique-se de ajustar para o tamanho de vetor da sua incorporação\n",
    "    metric=VectorMetric.COSINE,  # Similaridade por cosseno\n",
    "    check_exists=False,\n",
    ")\n",
    "print(f\"* Collection: {collection.full_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa648ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os documentos no Astra, associando os vetores de incorporação\n",
    "for doc in doc_splits:\n",
    "    embedding = embed_model.embed_query(doc.page_content)\n",
    "    document = {\n",
    "        \"text\": doc.page_content,\n",
    "        \"$vector\": embedding,\n",
    "    }\n",
    "    collection.insert(document)\n",
    "\n",
    "print(f\"* Documentos inseridos na coleção: {len(doc_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba7c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar consulta de similaridade\n",
    "query = \"llm agent memory\"\n",
    "query_embedding = embed_model.embed_query(query)\n",
    "\n",
    "results = collection.find(\n",
    "    sort={\"$vector\": query_embedding},\n",
    "    limit=5,  # Defina o limite de resultados desejado\n",
    "    include_similarity=True,\n",
    ")\n",
    "\n",
    "print(\"Resultados da pesquisa vetorial:\")\n",
    "for doc in results:\n",
    "    print(\"    \", doc[\"text\"], \" - Similaridade: \", doc[\"$similarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e91bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar o roteador para redirecionar a consulta\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|> Você é um especialista em encaminhar uma \n",
    "    pergunta do usuário para um vectorstore ou pesquisa na web. Use o vectorstore para perguntas sobre agentes LLM, \n",
    "    engenharia de prompt e ataques adversários. Você não precisa ser rigoroso com as palavras-chave \n",
    "    na pergunta relacionadas a esses tópicos. Caso contrário, use a pesquisa na web. Dê uma escolha binária 'web_search' \n",
    "    ou 'vectorstore' com base na pergunta. Retorne um JSON com uma única chave 'datasource' e \n",
    "    nenhum preâmbulo ou explicação. Pergunta para encaminhar: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roteamento da consulta baseado na pergunta\n",
    "start = time.time()\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "\n",
    "# Teste da cadeia de roteamento\n",
    "question = \"llm agent memory\"\n",
    "routing_decision = question_router.invoke({\"question\": question})\n",
    "print(f\"Decisão de roteamento: {routing_decision}\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"O tempo necessário para gerar resposta pela Router Chain em segundos: {end - start}\")\n",
    "\n",
    "# Resultado esperado: {'datasource': 'vectorstore'}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
